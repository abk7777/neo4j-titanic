{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/gregory/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from mordecai import Geoparser\n",
    "import pycountry\n",
    "\n",
    "# Download NLP data for country extraction // make this modular by setting nltk data path\n",
    "nltk.download('treebank')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('punkt') # Download corpora for GPE extraction\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Display options\n",
    "pd.options.display.max_rows = None  # display all rows\n",
    "pd.options.display.max_columns = None  # display all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path and import\n",
    "sys.path[0] = '../'\n",
    "data = pd.read_csv(sys.path[0] + '/data/clean/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of Country:[strings]\n",
    "# If row.`home.dest` is in map assign Country value\n",
    "\n",
    "abbrev_map = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    \"Alberta\": \"AB\",\n",
    "    \"British Columbia\": \"BC\",\n",
    "    \"Manitoba\": \"MB\",\n",
    "    \"New Brunswick\": \"NB\",\n",
    "    \"Newfoundland\": \"NL\",\n",
    "    \"Northwest Territories\": \"NT\",\n",
    "    \"Nova Scotia\": \"NS\",\n",
    "    \"Nunavut\": \"NU\",\n",
    "    \"Ontario\": \"ON\",\n",
    "    \"Prince Edward Island\": \"PE\",\n",
    "    \"Quebec\": \"PQ\",\n",
    "    \"Saskatchewan\": \"SK\",\n",
    "    \"Yukon\": \"YT\",\n",
    "    \"Northern Ireland\": \"NI\"}\n",
    "\n",
    "# Invert mappings\n",
    "abbrev_map = {v: k for k, v in abbrev_map.items()}\n",
    "\n",
    "# Apply mappings to replace state abbreviations\n",
    "data['home.dest'] = data['home.dest'].str.split().apply(lambda x: ' '.join([abbrev_map.get(word, word) for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used with Pandas apply method. Use batch version for better speed.\n",
    "def extract_country(row):\n",
    "    geo = Geoparser(country_threshold=0.9)\n",
    "    inferred = geo.geoparse(row)\n",
    "    country_range = range(len(inferred))\n",
    "    home_countries = set([inferred[i]['country_predicted'] for i in country_range])\n",
    "    home_countries = \", \".join(home_countries)\n",
    "    \n",
    "    return home_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished\n",
    "def batch_extract_country(series):\n",
    "    countries = []\n",
    "    geo = Geoparser()\n",
    "    batch = geo.batch_geoparse(series)\n",
    "    for doc_list in batch:\n",
    "        row = \", \".join(set([entry['country_predicted'] for entry in doc_list]))\n",
    "        countries.append(row)\n",
    "    \n",
    "    return pd.Series(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished\n",
    "def lookup_country_name(row):\n",
    "    if row == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        words = row.split(', ')\n",
    "        lookup = lambda country: pycountry.countries.lookup(country).name\n",
    "        names = list(map(lookup, words))\n",
    "        names = \", \".join(names)\n",
    "        return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 245/1309 [01:52<05:05,  3.48it/s] GET http://localhost:9200/geonames/_search [status:N/A request:10.141s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 384, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 2, in raise_from\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/http/client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/http/client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/http/client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "socket.timeout: timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 114, in perform_request\n",
      "    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 344, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\", line 686, in reraise\n",
      "    raise value\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/gregory/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 306, in _raise_timeout\n",
      "    raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10)\n",
      "100%|██████████| 1309/1309 [05:35<00:00,  3.90it/s] \n"
     ]
    }
   ],
   "source": [
    "# Parses home.dest to infer destination countries for each passenger\n",
    "# This step takes a while depending on the machine\n",
    "#data['home.country'] = batch_extract_country(data['home.dest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts ISO country code to country name\n",
    "data['home.country'] = data['home.country'].apply(lookup_country_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "#data.to_csv(sys.path[0] + 'data/clean/titanic_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
